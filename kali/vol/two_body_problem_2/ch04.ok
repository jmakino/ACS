= A Fourth-Order Integrator

== Coefficients

*Bob*: Now it is really time to show off my fourth-order integrator.

*Alice*: Can you show me again the code?

*Bob*: Here it is:

 :inccode:.rkbody.rb+rk4

*Alice*: That's quite a bit more complicated.  I see coefficients of
1/6 and 1/3 and 2/3.  How did you determine those?

*Bob*: I just looked them up from a book.  There are several ways to
choose those coefficients.  Runge-Kutta formulas form a whole family,
and the higher the order, the more choices there are for the
coefficients.  In addition, there are some extra simplifications that
can be made in the case of a second order differential equation where
there is no dependence on the first derivative.

*Alice*: As in the case of Newton's equation of motion, where the
acceleration is dependent on the position, but not on the velocity.

*Bob*: Exactly.  Here are the equations.  They can be found in that
famous compendium of equations and tables and graphs: <i>Handbook of
Mathematical Functions</i>, by M. Abramowitz and I. A. Stegun,
eds. [Dover, 1965].  You can even find the pages on the web now.
Here is what I copied from a web page:

link:rk4.gif

*Alice*: Quite a bit more complex than the second order one.

*Bob*: Yes, and of course it is written in a rather different form
that the way I chose to implement it.  I preferred to stay close to the
same style in which I had written the leapfrog and second-order Runge
Kutta.  This means that I first had to rewrite the Abramowitz and
Stegun expression.

== Really Fourth Order?

*Alice*: Let's write it out specifically, just to check what you did.
I know from experience how easy it is to make a mistake in these kinds
of transformations.

*Bob*: So do I!  Okay, the book starts with the differential equation:

:equation:
y''=f(x,y)

whereas we want to apply it to Newton's equation, which in a similar notation
would read

:equation:
\frac{d^2}{dt^2}\br = \ba(t,\br)

with the only difference that of course there is no specific time dependence
in Newton's equations of gravity, so we only have

:equation:
\frac{d^2}{dt^2}\br = \ba(\br)

To make a connection with Abramowitz and Stegun, their _x_ becomes our
time _t_, their _y_ becomes our position vector <tex>$\br$</tex>, and their
_f_ becomes our acceleration vector <tex>$\ba$</tex>.  Also, their
<tex>$y'$</tex>
becomes our velocity vector <tex>$\bv$</tex>.  Finally, their _h_ is
our time step +dt+.

*Alice*: Let us rewrite their set of equations in terms of the
dictionary you just provided.  And instead of +dt+ it is better to 
write <tex>$\Delta t$</tex>, since +dt+ is not an infinitesimal quantity
here, but a finite time interval.  This then gives us:

:eqnarray:
\br(t+\Delta t) &=& \br(t) + \Delta t\left(\bv(t) + {\textstyle {1 \over 6}}
              (\bk_1 + 2\bk_2)\right)                           \nonumber\\
\bv(t+\Delta t) &=& \bv(t) + {\textstyle {1 \over 6}}\bk_1 +
              {\textstyle {2 \over 3}}\bk_2 +
              {\textstyle {1 \over 6}}\bk_3                     \nonumber\\
\bk_1 &=& \Delta t\ba(\br(t))                                   \nonumber\\
\bk_2 &=& \Delta t\ba\left(\br(t) + {\textstyle {1 \over 2}}\Delta t\bv +
          {\textstyle {1 \over 8}}\Delta t\bk_1\right)          \nonumber\\
\bk_3 &=& \Delta t\ba\left(\br(t) + \Delta t\bv +
          {\textstyle {1 \over 2}}\Delta t\bk_2\right)          \nonumber

In order to make a connection with your code, we can define three
variables <tex>$\ba_0$</tex>, <tex>$\ba_1$</tex>, and <tex>$\ba_2$</tex>
as follows, taking the right hand sides of the last three equations above,
but without the <tex>$\Delta t$</tex> factor:

:eqnarray:
\ba_0 &=& \ba(\br(t))                                           \nonumber\\
\ba_1 &=& \ba\left(\br(t) + {\textstyle {1 \over 2}}\Delta t\bv +
          {\textstyle {1 \over 8}}\Delta t\bk_1\right)          \nonumber\\
\ba_2 &=& \ba\left(\br(t) + \Delta t\bv +
          {\textstyle {1 \over 2}}\Delta t\bk_2\right)          \nonumber

Comparing this with the definitions of <tex>$\bk_1$</tex>,
<tex>$\bk_2$</tex>, and <tex>$\bk_3$</tex>, we can eliminate all
<tex>$\bk$</tex> values from the right hand side:

:eqnarray:
\ba_0 &=& \ba(\br(t))                                           \nonumber\\
\ba_1 &=& \ba\left(\br(t) + {\textstyle {1 \over 2}}\Delta t\bv +
          {\textstyle {1 \over 8}}(\Delta t)^2\ba_0\right)        \nonumber\\
\ba_2 &=& \ba\left(\br(t) + \Delta t\bv +
          {\textstyle {1 \over 2}}(\Delta t)^2\ba_1\right)        \nonumber

And indeed, these are exactly the three variables <tt>a0</tt>, <tt>a1</tt>,
and <tt>a2</tt>, that are computed in your <tt>rk4</tt> method.

We can now rewrite the first two of Abramowitz and Stegun's equations as:

:eqnarray:
\br(t+\Delta t) &=& \br(t) + \Delta t\bv(t) +
              {\textstyle {1 \over 6}}(\Delta t)^2(\ba_0 + 2\ba_1)  \nonumber\\
\bv(t+\Delta t) &=& \bv(t) + {\textstyle {1 \over 6}}\Delta t(\ba_0 +
              4\ba_1 + \ba_2)                                       \nonumber

And these correspond precisely to the last two lines in your
<tt>rk4</tt> method.

*Bob*: Yes, that was how I derived my code, but I must admit, I did
not write it down as neatly and convincingly as you just did.

*Alice*: So yes, you have indeed implemented Abramowitz and Stegun's
fourth-order Runge Kutta scheme.  But wait a minute, what
you found here, equation 22.5.22, shows a right-hand side with a
stated energy error of order <tex>$h^4$</tex> -- which would suggest
that the scheme is is only third-order accurate.

*Bob*: Hey, that is right.  If you make <tex>$h$</tex> smaller,
the number of steps will go up according to <tex>$1/h$</tex>, and
therefore the total error for a give problem will grow proportionally
to <tex>$h^4.(1/h) \propto h^3$</tex>.  This would indeed imply that
this method is third-order.  But that would be very unusual.  In all
text books I've seen, you mostly come across second-order and
fourth-order Runge Kuttas.  While you certainly can construct a
third-order version, I wouldn't have expected Abramowitz and Stegun to
feature one.  Besides, look at the last expression for the velocity.
Doesn't that resemble Simpson's rule, suggesting fourth-order accuracy.
I'm really puzzled now.

*Alice*: Could it be a misprint?

*Bob*: Anything is possible, though here that is not very likely.  This
is such a famous book, that you would expect the many readers to have
debugged the book thoroughly.

*Alice*: Let's decide for ourselves what is true and what is not.

*Bob*: Yes.  And either way, whatever the outcome, it will be a good
exercise for the students.  Let's first test it numerically.

*Alice*: Fine with me.  But then I want to derive it analytically as well,
to see whether we really can understand the behavior of the numerical
results from first principles.

== Comparing the Four Integrators

*Bob*: Let us start again with a time step of 0.001, for a duration
of ten time units.

*Alice*: Just to compare, why don't you run them for all four schemes.

*Bob*: I am happy to do so.  And while we are not sure yet whether our
higher-order Runge-Kutta scheme is 3rd order or 4th order, let me
continue to call it 4th order, since I've called it <tt>rk4</tt> in my
code.  If it turns out that it is 3rd order, I'll go back and rename
it to be <tt>rk3</tt>.

*Alice*: Fair enough.

*Bob*: Okay: forward Euler, leapfrog, 2nd order R-K, and hopefully-4th
order R-K:

 :commandoutput: ruby integrator_driver2d.rb < euler.in

*Alice*: Ah, this is the result from the forward Euler algorithm,
because the fifth line of the output announces that this was the
integration method used.  As we have seen before, energy conservation
has larger errors.  We have an absolute total energy error of 0.425,
and a relative change in total energy of -0.486.  I see that you
indeed used <tt>dt = 0.001</tt> from what is echoed on the first line
in the output.  And yes, for these choices of initial conditions and
time step size we had seen that things went pretty badly.

*Bob*: The leapfrog does better:

 :commandoutput: ruby integrator_driver2e.rb < euler.in

*Alice*: Much better indeed, as we had seen before.  A relative energy
error of <tex>$-4\times10^{-7}$</tex> is great, an error much less
than one in a million.

*Bob*: Time for second-order Runge-Kutta:

 :commandoutput: ruby integrator_driver2f.rb < euler.in

*Alice*: Not as good as the leapfrog, but good enough.  What I'm curious
about is how your hopefully-fourth-order Runge-Kutta will behave.

*Bob*: Here is the answer:

 :commandoutput: ruby integrator_driver2g.rb < euler.in

*Alice*: What a difference!  Not only do we have much better energy
conservation, on the order of a few times <tex>$10^{-9}$</tex>,
also the positional accuracy is already on the level of a few times
<tex>$10^{-8}$</tex>, when we compare the output with our most
accurate previous runs.  And all that with hardly any waiting time.

*Bob*: Yes, I'm really glad I threw that integrator into the menu.
The second-order Runge-Kutta doesn't fare as well as the leapfrog, at
least for this problem, even though they are both second-order.  But
the hopefully-fourth-order Runge-Kutta wins hands down.

== Fourth Order Runge-Kutta

*Alice*: Let's make the time step ten times smaller:

 :commandoutput: ruby integrator_driver2h.rb < euler.in

*Bob*: What a charm!  Essentially machine accuracy.  This makes it
pretty obvious that fourth-order integrators win out hands down, in
problems like this, over second-order integrators.  Higher order
integrators may be even faster, but that is something we can explore
later.

*Alice*: Well done.  And, by the way, this does suggest that the scheme
that you copied from that book is indeed fourth-order.  It almost seems
better than fourth order.

*Bob*: Let me try a far larger time step, but a much shorter duration,
so that we don't have to integrate over a complicated orbit.  How
about these two choices.  First:

 :commandoutput: ruby integrator_driver2i.rb < euler.in

And then:

 :commandoutput: ruby integrator_driver2j.rb < euler.in

*Alice*: The time step went down by a factor 10, and the energy
conservation got better by a factor of almost exactly <tex>$10^4$</tex>.
We really seem to have a fourth order integration scheme.

*Bob*: This proves it.  And I can keep the name <tt>rk4</tt>,
since it lives up to its claim.  Good!

*Alice*: At least it comes close to living up to its name, and it
sort-of proves it.

== Snap, Crackle, and Pop

*Bob*: What do you mean with `comes close'?  What more proof would you
like to have?

*Alice*: I would still be happier if I could really prove it,
with pen and paper, rather than through the plausibility of numerical
experimentation.

*Bob*: Go ahead, if you like!

*Alice*: Let us take your implementation

 :inccode:.rkbody.rb+rk4

and let us determine analytically what the first five terms in a Taylor
series in +dt+ look like.  We can then see directly whether the error term
is proportional to <tex>$h^4$</tex>, as the book claims, or <tex>$h^5$</tex>,
as our calculations indicate.

To start with, let us look at your variable <tt>a2</tt>, which is the
acceleration after a time step +dt+, starting from an initial
acceleration <tt>a0</tt>.  A Taylor expansion can approximate
<tt>a2</tt> as:

:equation:
\label{a2}
\ba_2 = \ba_0 + {d\ba\over dt} \Delta t
              + {1 \over 2}{d^2\ba\over dt^2} (\Delta t)^2
              + {1 \over 6}{d^3\ba\over dt^3} (\Delta t)^3
              + {1 \over 24}{d^4\ba\over dt^4} (\Delta t)^4

where all the derivatives are understood to be evaluated at the same
time as <tex>$\ba_0$</tex>, namely at the beginning of our time step.

*Bob*: The first derivative of the acceleration, <tex>$d\ba/dt$</tex>,
is sometimes called the <i>jerk</i>.  How about the following notation:

:equation:
\bj = \frac{d^3}{dt^3} \br = \frac{d\ba}{dt}

*Alice*: Fine with me.  I believe the term `jerk' has crept into the
literature relatively recently, probably originally as a pun.  If a
car or train changes acceleration relatively quickly you experience
not a smoothly accelerating or decelerating motion, but instead a
rather `jerky' one.

*Bob*: It may be more difficult to come up with terms for the unevenness
in the jerk.

*Alice*: Actually, I saw somewhere that someone had used the words
_snap_, _crackle_, and _pop_, to describe the
next three terms.

*Bob*: As in the rice crispies?  Now that will confuse astronomers
who did not grow up in the United States!  If they haven't seen the
rice crispy commercials, they will have no clue why we would use those
names.  And come to think of it, I don't have much of a clue yet either.

:figure: scp.jpg 5cm scp
Snap, Crackle, and Pop

*Alice*: Ah, but the point is that the names of these three cheerful
characters lend themselves quite well to describe more-than-jerky behavior.
After all, the popping of a rice crispy is a rather sudden change of state.

*Bob*: Okay, now I get the drift of the argument.  A sudden snap comes to
mind, as a change in jerk.  And what changes its state more suddenly than
a snap?  Well, perhaps something that crackles, although that is pushing
it a bit.  But a pop is certainly a good word for something that
changes high derivatives of positions in a substantial way!

== An Attempt at an Analytical Proof

*Alice*: Okay, let's make it official:

:eqnarray:
\bs &=& \frac{d^4}{dt^4} \br \quad = \quad \frac{d^2}{dt^2} \ba \nonumber\\
\bc &=& \frac{d^5}{dt^5} \br \quad = \quad \frac{d^3}{dt^3} \ba \nonumber\\
\bp &=& \frac{d^6}{dt^6} \br \quad = \quad \frac{d^4}{dt^4} \ba

Which turns my earlier equation into:

:equation:
\ba_2 = \ba_0 + \bj_0 \Delta t
              + {\textstyle {1 \over 2}}\bs_0 (\Delta t)^2
              + {\textstyle {1 \over 6}}\bc_0 (\Delta t)^3
              + {\textstyle {1 \over 24}}\bp_0 (\Delta t)^4

*Bob*: Much more readable.

*Alice*: And your other variable <tt>a1</tt>, which indicates the acceleration
after only half a time step, now becomes:

:equation:
\ba_1 = \ba_0 + {\textstyle {1 \over 2}}\bj_0 \Delta t
              + {\textstyle {1 \over 8}}\bs_0 (\Delta t)^2
              + {\textstyle {1 \over 48}}\bc_0 (\Delta t)^3
              + {\textstyle {1 \over 384}}\bp_0 (\Delta t)^4

*Bob*: Ah, and now we can evaluate the last two lines in my <tt>rk4</tt>
method:

 :inccode:.rkbody.rb-1

*Alice*: Yes.  These two lines translate to:

:eqnarray:
\br_1 &=& \br_0 + \bv_0 \Delta t
          + {\textstyle {1 \over 6}}(\ba_0+2\ba_1)(\Delta t)^2 \nonumber\\
\bv_1 &=& \bv_0 + {\textstyle {1 \over 6}}(\ba_0+4\ba_1+\ba_2)\Delta t

Upon substitution the first line becomes:

:eqnarray:
\br_1 &=& \br_0 + \bv_0 \Delta t
          + {\textstyle {1 \over 6}}\ba_0(\Delta t)^2
          + {\textstyle {1 \over 3}}\ba_1(\Delta t)^2 \nonumber\\
&=& \br_0 + \bv_0 \Delta t
          + {\textstyle {1 \over 2}}\ba_0(\Delta t)^2
          + {\textstyle {1 \over 6}}\bj_0 (\Delta t)^3
          + {\textstyle {1 \over 24}}\bs_0 (\Delta t)^4
          + {\textstyle {1 \over 144}}\bc_0 (\Delta t)^5
          + O((\Delta t)^6) \nonumber

And the second line becomes:

:eqnarray:
\bv_1 &=& \bv_0
          + {\textstyle {1 \over 6}}\ba_0\Delta t
          + {\textstyle {2 \over 3}}\ba_1\Delta t
          + {\textstyle {1 \over 6}}\ba_2\Delta t \nonumber\\
&=& \bv_0 + \ba_0\Delta t
          + {\textstyle {1 \over 2}}\bj_0 (\Delta t)^2
          + {\textstyle {1 \over 6}}\bs_0 (\Delta t)^3
          + {\textstyle {1 \over 24}}\bc_0 (\Delta t)^4
          + {\textstyle {5 \over 576}}\bp_0 (\Delta t)^5
          + O((\Delta t)^6) \nonumber

*Bob*: Very nice.  In both cases the terms up to <tex>$(\Delta t)^4$</tex> are
perfectly correct, and the error starts in the <tex>$(\Delta t)^5$</tex> term,
where the Taylor series coefficient would have been <tex>${1 \over 120}$</tex>.
So the leading error terms are:

:eqnarray:
\delta \br_1 &=& {\textstyle -{1 \over 720}}\bc_0 (\Delta t)^5 \nonumber\\
\delta \bv_1 &=& {\textstyle {1 \over 2880}}\bp_0 (\Delta t)^5 \nonumber

This proves that the <tt>rk4</tt> Runge-Kutta method is truly fourth-order,
and that there was a typo in Abramowitz and Stegun.

*Alice*: Or so it seems.  But I must admit, I'm not completely comfortable.

== Shocked

*Bob*: Boy, you're not easy to please!  I was ready to declare victory
when we had our numerical proof, and you convinced me to do the additional
work of doing an analytical check.  If it were up to me, I would believe
numerics over analytics, any day.  Seeing is believing, the proof is
in the pudding, and all that.

*Alice*: Well, it _really_ is better to have _both_ a numerical
demonstration _and_ an analytical proof.  It is just too easy to be
fooled by coincidences or special cases, in numerical demonstrations.
And at the same time it is also easy to make a mistake in doing
analytical checks.  So I think it is only wise to do both, especially
with something so fundamental as an N-body integrator.

And I'm glad we did.  But something doesn't quite feel right.  Let me
just review what we have done.  We first expanded the orbit in a Taylor
series, writing everything as a function of time.  Then we showed that
the orbit obeyed the equations of motion, and . . . .

*Bob*: . . . and then you looked shocked.

*Alice*: Not only do I look shocked, I _am_ shocked!  How could we make
such a stupid mistake!?!

*Bob*: I beg your pardon?

*Alice*: We _assumed_ that we were sliding along the perfect orbit, and
then we _proved_ that we were sliding at the right pace, but we never
considered that our numerical orbit is only approximate, and that it
will have errors in _space_ as well as in time.  So we proved only half
of what we should have proved.  In other words, we haven't yet proven
anything at all, really!
