= Orbit vs. Interaction

== Space and Time Dependent Interactions

*Bob*: So you claim that our somewhat tedious proof isn't ironclad yet?
You said something about having checked time variations but not space
variations.  Could you explain that in a bit more detail?  I'm afraid
I didn't quite get it.

*Alice*: I'm still grappling with it myself.  This stuff is not easy.
No wonder people can spend a life time on designing numerical methods.
Linear and quadratic methods are not too hard, since you can still
sketch on a piece of paper what is going on.  But by the time you're
dealing with a fourth-order or higher method, you can no longer use
your intuition.  And when you switch to formal proofs, it is easy to
overlook things.  I'm sure we've done just that, but let me see whether
I can really put my finger on it.

What I meant was that any differential equation in a dynamical system
has a left-hand side that tells us how the orbit of a particle changes
in time as a result of interactions with the rest of the world.  The
dynamics is specified on the right-hand side, while the left-hand side
shows only the kinematics.  In other words, the physical forces can be
found on the right-hand side, while the mathematical description of the
deflection of the orbit, as a result of those forces, is given on the
left-hand side.

*Bob*: I find it always easier to look at a specific example.  Here
we have our Newtonian equations of motion, for the gravitational two-body
problem.

 :equation:
\frac{d^2}{dt^2}{\bf r} \, = \,
 - \,G\, \frac{M_1 + M_2}{r^3}{\bf r}

Indeed, the physical coupling constant _G_ appears only on the right-hand
side, where the gravitational interaction is specified, while the
left-hand side tells us how the orbit is bent: non-zero acceleration
describes the deviation of uniform rectilinear motion.  Okay, I'm
still with you.

*Alice*: Now the trick is to make explicit what we normal vaguely understand,
but rarely write down precisely, namely the space and time dependence of
the interaction.  In any N-body system, whether we have two particles
or a million, each particle feels a time-varying force field.

*Bob*: And that is what makes it non-trivial to do an N-body simulation.
If we only had to integrate the orbit of a particle rolling through a
fixed potential landscape, life would be a lot easier.  The problem is
that everything changes all the time: one moment a particle is coasting
along happily, and before you know it, suddenly it crosses the orbit
of another particle, resulting is a sharp deflection.

*Alice*: So let's write that down in a general way, not worrying about any
details of the actual interaction, but just making clear that the
interaction is both space and time dependent:

<tex>$$
\frac{d^2}{dt^2}{\bf r} \, = \, {\bf f}(\br, t)
$$</tex>

*Bob*: Meaning that on the left you have the acceleration and on the right
you have the force.

*Alice*: Not quite: if we denote the actual force by <tex>${\bf F}$</tex>,
we should write:

<tex>$$
m\frac{d^2}{dt^2}{\bf r} \, = \, {\bf F}(\br, t)
$$</tex>

which for the acceleration would give

<tex>$$
\frac{d^2}{dt^2}{\bf r} \, = \, {1\over m}{\bf F}(\br, t)
$$</tex>

where <tex>$m$</tex> is the mass of the particle; or in the case of
two-body motion, the reduced mass associated with the motion of the
equivalent one-body system, for a pseudo-particle in a time varying
force field.

Anyway, those details are irrelevant here.  My point is that I prefer
to talk about orbit vs. interaction, not orbit vs. force calculation.

*Bob*: In stellar dynamics, at least, people talk about force calculation
all the time.

*Alice*: I know, and then they write down the acceleration.  We've seen
already how easy it is to get into trouble when forgetting to take the
mass into account properly.

*Bob*: Well, yes, I can't argue with that, I'm afraid, seeing how I
blundered there, a while ago.

*Alice*: So let us call <tex>${\bf f}(\br, t)$</tex> the general interaction
term that causes an acceleration to occur.  That is in line with the notation
in books about general differential equations.  Abramowitz and Stegun used
the same notation too.

== The One-Dimensional Case

*Bob*: Now that we have settled nomenclature, can we look at what you claimed
went wrong with our derivation?

*Alice*: My point is that we have to be really explicit about the 
space and time dependence of the interaction terms, as follows:

<tex>$$
\frac{d^2}{dt^2}{\bf r} \, = \, {\bf f}(\br, t)\, = \, {\bf f}(\br(t), t)
$$</tex>

*Bob*: This is what we did.  We considered the time dependence of the orbit,
and how that fed back into the time dependence of the interaction term.

*Alice*: Yes, and we looked at the error term in terms of time dependence.
But we did not check the other contribution to the error in the
position, stemming from the fact that our orbit makes errors in space
as well as in time.

Eq. (ref(a2)) for <tex>$\ba_2$</tex>, for example, is written in terms
of _orbit_ variables, using only concepts from the left-hand side of
the differential equations that we are trying to solve.  But if you
look at your code, just above that equation, you see that the term
<tt>a2</tt> in your code is a call to the method +acc+, which does an
_interaction_ calculation: it invokes the right-hand side of the
differential equation.

*Bob*: I guess that that was not quite consistent.  How do you propose to
remedy our analysis?

*Alice*: If we _really_ wanted to be formally correct, we could track
the four-dimensional variations in space and time, taking into account
the three-dimensional spatial gradient of the interaction terms.

*Bob*: But from the way you just said that, I figure that that would be
too much, even for you.  I'm glad to hear that even you have your limits
in wanting to check things analytically!

*Alice*: I sure have.  And also, I don't think such a formal treatment
would be necessary.  If we can prove things in one dimension, I'm convinced
that we could rewrite our prove to three dimensions without too many problems,
albeit with a lot more manipulations.

*Bob*: You won't hear me arguing against simplification!  But I must admit,
you have made me curious.  Go on!

*Alice*: What I propose is that we go back to the way Abramowitz and Stegun
formulated their Runge-Kutta version.  After all, the whole discussion was
about whether their claim of having a fourth-order error was a misprint or not.
If we can prove that their method, as they stated it, is fourth-order accurate,
with an error that scales like the fifth power of the time step, I'm satisfied.

== A Change in Notation

*Bob*: Same for me.  But I propose that we use a somewhat more intuitive
notation than what they gave us.  To use _x_ to indicate time and _y_
to indicate position is confusing, to say the least.  Let us look at
the text again:

link:rk4.gif

How about the following translation, where the position _r_, velocity _v_,
and _a_ are all scalar quantities, since we are considering an effectively
one-dimensional problem:

 :eqnarray:
x \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,t     \nonumber\\
y \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,r     \nonumber\\
f(x,y) \,\,\,\,\, &\rightarrow& \,\,\,f(r,t)                       \nonumber\\
y' = {dy \over dx}  \,\,\, &\rightarrow& \,\,\,
\dot r = {dr \over dt} = v                                         \nonumber\\
y'' = {d^2y \over dx^2}  &\rightarrow& \,\,\,\,
\ddot r = {d^2r \over dt^2} = {dv\over dt} = a                     \nonumber\\
h \,\,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow&  \Delta t = \tau        \nonumber\\
k_i \,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\, k_i \tau   \nonumber\\
n \,\,\,\,\,\,\,\,\,\,\,\, &\rightarrow& \,\,\,\,\,\,\,\,\,\, 0    \nonumber

The equations, as proposed by Abramowitz and Stegun, then become:

 :eqnarray:
\label{base}
\ddot r\,&=&\,f(r, t)                                              \nonumber\\
&&                                                                 \nonumber\\
r_1 \,&=&\, r_0 + 
\left(v_0+\one{6}\left(k_1+2k_2\right)\tau\right)\tau              \nonumber\\
v_1 \,&=&\, v_0 + 
\one{6}\left(k_1+4k_2+k_3\right)\tau                               \nonumber\\
&&                                                                 \nonumber\\
k_1\,&=&\,f(r_0, 0)                                                \nonumber\\
k_2\,&=&\,f(r_0 + \hhalf v_0\tau + \one{8}k_1\tau^2, \hhalf\tau)   \nonumber\\
k_3\,&=&\,f(r_0 + v_0\tau + \hhalf k_2\tau^2, \tau)

== The Auxiliary _k_ Variables

*Alice*: Yes, I find that notation much more congenial, and I'm glad to see
the explicitly space and time dependence now in the definitions of the three
_k_ variables.

Let us work out what these _k_ variables actually look like, when we
substitute the orbit dependence in the right hand side.  What we see
there is an interesting interplay between a physical interaction term
_f_ that has as an argument the mathematical orbit <tex>$r(t)$</tex>.

Since you have introduced <tex>$\tau$</tex> as the increment in time,
starting from time zero, let me introduce the variable <tex>$\rho_1$</tex>
to indicate the increment in space, the increment in position at the time
that we evaluate <tex>$k_1$</tex>

:equation:
\label{rho1}
\rho_1 \,=\, \hhalf v_0\tau + \one{8}k_1\tau^2

For consistency, let me also use

:equation:
\tau_1 \,=\, \hhalf \tau

for the increment in time, when we evaluate <tex>$k_1$</tex>.
Let me also abbreviate <tex>$f_0 = f(r_0,0)$</tex>.

What I propose to do now is to expand <tex>$k_1$</tex> in a double Taylor
series, simultaneously in space and in time.  This will hopefully repair
the oversight we made before, when we only considered a time expansion.

*Bob*: How do you do a two-dimensional Taylor series expansion?

*Alice*: Whenever you have mixed derivatives, such as

:equation:
{\partial^3 f \over \partial r^2\partial t} \,=\, f_{rrt}

you multiply the coefficients you would give to each dimension separately;
in this case you would get a factor of one half, because of the second
partial derivative with respect to space; the single time derivative
just gives a factor one.  You can easily check how this work when you
look at a two-dimensional landscape, and you imagine how you pick up
difference in height by moving North-South and East-West.  Anyway, here
is the concrete example, for our case, where I use the abbreviation that
I introduced above, with partial differentiation indicated with subscripts:

 :eqnarray:
k_2 \,=\, f_0 &+&\, f_r\rho_1 + f_t\tau_1                           \nonumber\\
&+&\,\hhalf f_{rr}\rho_1^2+f_{rt}\rho_1\tau_1+\hhalf f_{tt}\tau_1^2 \nonumber\\
&+&\,\one{6}f_{rrr}\rho_1^3+\hhalf f_{rrt}\rho_1^2\tau_1
     +\hhalf f_{rtt}\rho_1\tau_1^2 + \one{6}f_{ttt}\tau_1^3

where I have grouped the different orders of expansion in terms of small
variables on different lines.  Using the fact that
<tex>$\tau_1 = \hhalf \tau$</tex>, we can also write this as

 :eqnarray:
\label{k2}
k_2 \,=\, f_0 &+&\, f_r\rho_1 + \hhalf f_t\tau                    \nonumber\\
&+&\,\hhalf f_{rr}\rho_1^2+
\hhalf f_{rt}\rho_1\tau+\one{8}f_{tt}\tau^2                       \nonumber\\
&+&\,\one{6}f_{rrr}\rho_1^3+\one{4}f_{rrt}\rho_1^2\tau
     +\one{8}f_{rtt}\rho_1\tau^2 + \one{48}f_{ttt}\tau^3

Similarly, we can introduce

:equation:
\label{rho2}
\rho_2 \,=\, v_0\tau + \hhalf k_2\tau^2

as the spatial offset in the argument for the interaction term for the
calculation of <tex>$k_3$</tex>, for which we get:

 :eqnarray:
\label{k3}
k_3 \,=\, f_0 &+&\, f_r\rho_2 + f_t\tau                           \nonumber\\
&+&\,\hhalf f_{rr}\rho_2^2+f_{rt}\rho_2\tau+\hhalf f_{tt}\tau^2   \nonumber\\
&+&\,\one{6}f_{rrr}\rho_2^3+\hhalf f_{rrt}\rho_2^2\tau
     +\hhalf f_{rtt}\rho_2\tau^2 + \one{6}f_{ttt}\tau^3

Now the next step is to substitute the definitions of the
<tex>$\rho_i$</tex> variables in the <tex>$k_{i+1}$</tex> equations,
and to keep terms up to the third order in <tex>$\tau$</tex>.

== The Next Position

*Bob*: That's going to be quite messy.

*Alice*: Well, yes, but we have no choice.  Let me start with
the substitution indicated in Eq. (ref(rho1)), where we may as well
replace <tex>$k_1$</tex> by its value <tex>$f_0$</tex>:

:equation:
\label{rho1s}
\rho_1 \,=\, \hhalf v_0\tau + \one{8}f_0\tau^2

which is just its Taylor series expansion in time, up to second order.
Using this substitution in Eq. (ref(k2)), we get:

 :eqnarray:
\label{k2s}
k_2 \,=\, f_0 &+&\, \left\{
\hhalf v_0 f_r + \hhalf f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
\one{8}f_0f_r + \one{8}v_0^2f_{rr}
+\one{4}v_0f_{rt}+\one{8}f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \left\{
\one{16}v_0f_0f_{rr} + \one{16}f_0f_{rt} + \right.                \nonumber\\
&& \ \ \left. \one{48}v_0^3f_{rrr}
+ \one{16}v_0^2f_{rrt}+ \one{16}v_0f_{rtt}+ \one{48}f_{ttt}
\right\} \tau^3

Similarly, we have to take Eq. (ref(k3)), where we have to use the
substitution indicated in Eq. (ref(rho2)), which we can rewrite, using
the expression for <tex>$k_2$</tex> that we just derived, up till third
order in <tex>$\tau$</tex>, as;

:equation:
\label{rho2s}
\rho_2 \,=\, v_0\tau + \hhalf f_0\tau^2 + \left\{
\hhalf v_0 f_r + \hhalf f_t \right\} \tau^3

*Bob*: And now I see concretely what you meant, when you said that our
previous attempt at proving the fourth-order nature of the Runge-Kutta
integrator neglected the treatment of the right-hand side of the
differential equation, of the physical interaction terms.  They would
not have showed up if we were trying to prove the correctness of a
second-order Runge-Kutta scheme.  It is only at third order in
<tex>$\tau$</tex> that they appear, in the form of partial derivatives
of the interaction term.

*Alice*: Yes, that must be right.  Okay, ready for a next round of
substitution?  Eq. (ref(k3)) then becomes:

 :eqnarray:
\label{k3s}
k_3 \,=\, f_0 &+&\, \left\{
v_0 f_r + f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
\hhalf f_0f_r + \hhalf v_0^2f_{rr}
+v_0f_{rt}+\hhalf f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \Big\{
\one{4}v_0\left(f_r\right)^2 + \one{4}f_rf_t +
\hhalf v_0f_0f_{rr} + \hhalf f_0f_{rt} +                \nonumber\\
&& \ \ \one{6}v_0^3f_{rrr}
+ \hhalf v_0^2f_{rrt}+ \hhalf v_0f_{rtt}+ \one{6}f_{ttt}
\Big\} \tau^3

We are now in a position to evaluate the solution that our Runge-Kutta
scheme gives us for the value of the position and velocity in the next
time step, as given in Eq. (ref(base)), since there we can now
substitute <tex>$k_2$</tex> and <tex>$k_3$</tex> in the expressions for
<tex>$r_1$</tex> and <tex>$v_1$</tex>.

Starting with the position, we find:

 :eqnarray:
\label{r1}
r_1 \,=\, r_0 &+&\, v_0\tau + \hhalf f_0\tau^2 + \one{6}\left\{
v_0 f_r + f_t \right\} \tau^3                                      \nonumber\\
&+&\, \one{24}\left\{
f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}
\right\} \tau^4

== The Next Velocity

*Bob*: And the last term in curly brackets, in the first line of the
last equation must be the jerk.  I begin to see some structure in these
expressions.

*Alice*: Yes.  But let me get the velocity as well, and then we can take
stock of the whole situation.  Let's see.  To solve
for <tex>$v_1$</tex> in Eq. (ref(base)), we need to use the following
combination of <tex>$k_i$</tex> values:

 :eqnarray:
4k_2+k_3 \,=\, 5f_0 &+&\, \left\{
3v_0 f_r + 3f_t
\right\} \tau                                           \nonumber\\
&+&\, \left\{
f_0f_r + v_0^2f_{rr}
+2v_0f_{rt}+ f_{tt}
\right\} \tau^2                                           \nonumber\\
&+&\, \Big\{
\one{4}v_0\left(f_r\right)^2 + \one{4}f_rf_t +
{\textstyle\frac{3}{4}} v_0f_0f_{rr} + {\textstyle\frac{3}{4}} f_0f_{rt}
+   \nonumber\\
&& \ \ \,\one{4}v_0^3f_{rrr}
+ {\textstyle\frac{3}{4}} v_0^2f_{rrt}+ {\textstyle\frac{3}{4}} v_0f_{rtt}
+ \one{4}f_{ttt} \Big\} \tau^3                                     \nonumber

Plugging this back into the expression for <tex>$v_1$</tex> in Eq. (ref(base)),
we get:

 :eqnarray:
\label{v1}
v_1 \,=\, v_0 &+&\, f_0\tau + \hhalf \left\{
v_0 f_r + f_t \right\} \tau^2                                      \nonumber\\
&+&\, \one{6}\left\{
f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}
\right\} \tau^3                                                    \nonumber\\
&+&\, \one{24}\Big\{
v_0\left(f_r\right)^2 + f_rf_t +
3v_0f_0f_{rr} + 3f_0f_{rt} \nonumber\\
&& \ \ \ \ \ \ v_0^3f_{rrr}
+ 3v_0^2f_{rrt}+ 3v_0f_{rtt}
+ f_{ttt} \Big\} \tau^4

*Bob*: Quite an expression.  Well done!

*Alice*: And an expression that makes sense: we see that indeed the
velocity, Eq. (ref(v1)),
is the time derivative of the position, Eq. (ref(r1)), up to the order given
there.  So everything is consistent.

== Everything Falls into Place

*Bob*: So where do we stand now?  This was all nice and fine, but I must
admit, I'm beginning to lose the forest for the trees.

*Alice*: What we have done now is to compute the predicted positions and
velocities after one time step, and we have checked that if we vary the
length of the time step, we indeed find that the velocity changes like the
time derivative of the position.  The question is: are they really solutions
of the equations of motion that we started out with, up to the fourth order
in the time step size?

Let us go back to the equations of motion, in the form we gave them
just after we translated the expressions from Abramowitz and Stegun:

:equation:
a(t) = \ddot r(t)\,=\,f(r(t), t)

For time zero this gives:

:equation:
a_0 = f_0

We can differentiate the equation for <tex>$a(t)$</tex>, to find the jerk:

:equation:
j(t) \,=\, {d\over dt}f(r(t), t) \,=\, f_r {dr\over dt} + f_t 
\,=\, v(t)f_r + f_t

which gives us at the starting time:

:equation:
j_0 \,=\, v_0 f_r + f_t

When we differentiate the equation for <tex>$j(t)$</tex>, we find the snap:

:equation:
s(t) \,=\, {d\over dt}j(t) \,=\, a(t)f_r + \left(v(t)\right)^2f_{rr}
+ 2v(t)f_{rt} + f_{tt}

or at time <tex>$t=0$</tex> :

:equation:
s_0 \,=\, f_0f_r + v_0^2f_{rr} +2v_0f_{rt}+ f_{tt}

Next, with one more differentiation of the equation for <tex>$s(t)$</tex>,
we can derive the crackle:

:equation:
c(t) \,=\, {d\over dt}s(t) \,=\, j(t)f_r + 3v(t)a(t)f_{rr} + 3a(t)f_{rt} 
+ \left(v(t)\right)^3f_{rrr} + 3\left(v(t)\right)^2f_{rrt}
+ 3v(r)f_{rtt} + f_{ttt}

Using the expression for the jerk, that we found above, we find for
time zero:

:equation:
c_0 \,=\, v_0\left(f_r\right)^2 + f_rf_t + 3v_0f_0f_{rr} + 3f_0f_{rt} 
+ v_0^3f_{rrr} + 3v_0^2f_{rrt}+ 3v_0f_{rtt} + f_{ttt} \ \ \ \ 

*Bob*: I see, now the clouds are clearing, and the sun is shining through!
When you substitute these values back into the equation for the new position,
Eq. (ref(r1)), we find:

:equation:
r_1 \,=\, r_0 +\, v_0\tau + \hhalf a_0\tau^2 + \one{6}j_0\tau^3
+ \one{24}s_0\tau^4

and similarly, for Eq. (ref(v1)), we find:

:equation:
v_1 \,=\, v_0 +\, a_0\tau + \hhalf j_0\tau^2 + \one{6}s_0\tau^3
+ \one{24}c_0\tau^4

Both are nothing else but the Taylor series for their orbits.

*Alice*: And now we have derived them in a fully space-time dependent
way.

*Bob*: Congratulations!  I'm certainly happy with this result.  But I must
admit, I do wonder whether this conclusion would satisfy a
mathematician working in numerical analysis.

*Alice*: Probably not.  Actually, I hope not.  I'm glad some people are more
careful than we are, to make sure that our algorithms really do what
we hope they do.  At the same time, I'm sure enough now that we have a
reasonable understanding of what is going on, and I'm ready to move on.

*Bob*: I certainly wouldn't ask my students to delve deeper into this matter
than we have been doing so far.  At some point they will have to get results
from their simulations.

*Alice*: At the same time, unless they have a fair amount of understanding
of the algorithms that they rely on, it will be easy for them to use those
algorithms in the wrong way, or in circumstances where their use is
not appropriate.  But I agree, too much of a good thing may simply be
too much.  Still, if a student would come to be with a burning curiosity
to find out more about what is _really_ going on with these
higher-order integration methods, I would encourage that him or her to
get deeper into the matter, through books or papers in numerical analysis.

*Bob*: I don't think that is too likely to happen, but if such students
appear on my doorsteps, I'm happy to send them to you!

*Alice*: And I would be happy too, since I might well learn from them.
I've the feeling that we've only scratched the surface so far.

== Debugging All and Everything?

*Bob*: Yes, and our scratching had some hits and some misses.
If we are ever going to write up all this exploring we are doing now,
for the benefit of our students, we have to decide what to tell them, and
what to sweep under the rug.

*Alice*: I suggest we tell them everything, the whole story of how and where
we went wrong.  If we're going to clean up everything, we will wind up with
a text book, and text books there are already enough of in this world.

*Bob*: I wasn't thinking about any formal publication, good grief, that would
be far too much work!  I was only thinking about handing out notes to the
students in my class.  And I must say, I like the idea of showing them
a look into our kitchen.  I think it will be much more interesting for the
students, and after all, if you and I are making these kinds of mistakes,
we can't very well pretend that the students would be above making those
types of mistakes, can we?

*Alice*: Precisely.  They may learn from our example, in two ways: they will
realize that everyone makes mistakes all the time, and that every mistake
is a good opportunity to learn more about the situation at hand.  Debugging
is not only something you have to do with computer programs.  When doing
analytical work, you also have to debug both the equations you write down,
in which you will make mistake, as well as the understanding you bring to
the writing down in the first place.

You might even want to go so far as to say that life is one large debugging
process.  I heard Gerald Sussman saying something like that, some day.  He
was in a talkative mood, as usual.  Somebody asked him what he meant
when he said that he considers himself to be a philosophical engineer, and
that was his answer.

*Bob*: I doubt that I will have to debug my beer, I sure hope not!  But as
far as codes and math and physics, sure, I'd buy that.  So yes, let us
present the students our temporary failures on the level of math as well
as on the level of coding.  I'm all for it!

*Alice*: So, what's next?  A sixth order integrator?

*Bob*: I hadn't thought about that, but given the speed-up we got with
going from <tt>rk2</tt> to <tt>rk4</tt>, it would be interesting to
see whether this happy process will continue when we would go to a
<tt>rk6</tt> method, for a sixth-order Runge-Kutta method.  The
problem is that Abramowitz and Stegun don't go beyond fourth order
methods.  Hmm.  I'll look around, and see what I can find.
